{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suffering-festival",
   "metadata": {
    "id": "suffering-festival"
   },
   "source": [
    "# Pixel classification using Scikit image\n",
    "Pixel classification is a technique for assigning pixels to multiple classes. If there are two classes (object and background), we are talking about binarization. In this example we use a [random forest classifier](https://en.wikipedia.org/wiki/Random_forest) for pixel classification.\n",
    "\n",
    "See also\n",
    "* [Scikit-image random forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "* [Classification of land cover by Chris Holden](https://ceholden.github.io/open-geo-tutorial/python/chapter_5_classification.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ac9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-image\n",
    "# !pip install matplotlib\n",
    "# !pip install nd2\n",
    "# !pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extreme-papua",
   "metadata": {
    "id": "extreme-papua"
   },
   "outputs": [],
   "source": [
    "from skimage.io import imsave, imread\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a685572",
   "metadata": {},
   "source": [
    "# Define input path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91fb3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the folder containing the images of a certain channel\n",
    "\n",
    "filepath = '/media/eric/WD_Elements/EPFL-data/Lucas/'\n",
    "\n",
    "fn_orange = 'orange_20230920_AirGel_WTmSc_dwspFmN_mix_LAM02J-L_CIP_5.5hpi_23.5h.nd...el_WTmSc_dwspFmN_mix_LAM02J-L_CIP_5.5hpi_23.5h.nd2 (series 08) - C=1.tif'\n",
    "fn_green = 'green_20230920_AirGel_WTmSc_dwspFmN_mix_LAM02J-L_CIP_5.5hpi_23.5h.nd...el_WTmSc_dwspFmN_mix_LAM02J-L_CIP_5.5hpi_23.5h.nd2 (series 08) - C=0.tif'\n",
    "\n",
    "\n",
    "orange = filepath + fn_orange\n",
    "green = filepath + fn_green\n",
    "\n",
    "# if orange endswith .nd2 then import pims_nd2\n",
    "if orange.endswith('.nd2'):\n",
    "    import pims_nd2 as pims\n",
    "\n",
    "else:\n",
    "    import pims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c577c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "%gui qt\n",
    "\n",
    "def Draw_and_Save(img,output_dir,output_fn):\n",
    "    viewer = napari.Viewer()\n",
    "    viewer.add_image(img)\n",
    "    labels = viewer.add_labels(np.zeros(img.shape,dtype='uint8'))\n",
    "\n",
    "\n",
    "    viewer.show(block=True)\n",
    "\n",
    "    # get the drawings first\n",
    "    manual_annotations = labels.data\n",
    "\n",
    "    # save the drawings\n",
    "    manual_labels_filename = output_dir + '/' + output_fn\n",
    "    imsave(manual_labels_filename, manual_annotations, check_contrast=False)\n",
    "    viewer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bdcde9",
   "metadata": {},
   "source": [
    "# Prepare GT and save it together with the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55b907a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../data_3D/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26269504",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/eric/Desktop/GitHub/CompetitionAssay/CompetitionAssay/RFC_training_3D.ipynb Cell 9\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eric/Desktop/GitHub/CompetitionAssay/CompetitionAssay/RFC_training_3D.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(output_dir \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgreen/RAW\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eric/Desktop/GitHub/CompetitionAssay/CompetitionAssay/RFC_training_3D.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(output_dir \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgreen/RAW\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/eric/Desktop/GitHub/CompetitionAssay/CompetitionAssay/RFC_training_3D.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m orange_seq \u001b[39m=\u001b[39m pims\u001b[39m.\u001b[39;49mopen(orange)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eric/Desktop/GitHub/CompetitionAssay/CompetitionAssay/RFC_training_3D.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m green_seq \u001b[39m=\u001b[39m pims\u001b[39m.\u001b[39mopen(green)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eric/Desktop/GitHub/CompetitionAssay/CompetitionAssay/RFC_training_3D.ipynb#X11sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m Z_slices \u001b[39m=\u001b[39m \u001b[39m19\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/devbio-napari-env/lib/python3.9/site-packages/pims/api.py:162\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(sequence, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(sequence, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    131\u001b[0m     \u001b[39m\"\"\"Read a filename, list of filenames, or directory of image files into an\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m    iterable that returns images as numpy arrays.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39m    >>> frame_shape = video.frame_shape # Pixel dimensions of video\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     files \u001b[39m=\u001b[39m glob\u001b[39m.\u001b[39;49mglob(sequence)\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(files) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# todo: test if ImageSequence can read the image type,\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39m#       delegate to subclasses as needed\u001b[39;00m\n\u001b[1;32m    166\u001b[0m         \u001b[39mreturn\u001b[39;00m ImageSequence(sequence, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/devbio-napari-env/lib/python3.9/glob.py:22\u001b[0m, in \u001b[0;36mglob\u001b[0;34m(pathname, recursive)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mglob\u001b[39m(pathname, \u001b[39m*\u001b[39m, recursive\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     12\u001b[0m     \u001b[39m\"\"\"Return a list of paths matching a pathname pattern.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[39m    The pattern may contain simple shell-style wildcards a la\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m    zero or more directories and subdirectories.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(iglob(pathname, recursive\u001b[39m=\u001b[39;49mrecursive))\n",
      "File \u001b[0;32m~/miniconda3/envs/devbio-napari-env/lib/python3.9/glob.py:47\u001b[0m, in \u001b[0;36m_iglob\u001b[0;34m(pathname, recursive, dironly)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m dironly\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m basename:\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mlexists(pathname):\n\u001b[1;32m     48\u001b[0m         \u001b[39myield\u001b[39;00m pathname\n\u001b[1;32m     49\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[39m# Patterns ending with a slash should match only directories\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/devbio-napari-env/lib/python3.9/posixpath.py:177\u001b[0m, in \u001b[0;36mlexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39m\"\"\"Test whether a path exists.  Returns True for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     os\u001b[39m.\u001b[39;49mlstat(path)\n\u001b[1;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m    179\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# create output directory\n",
    "import os\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if not os.path.exists(output_dir + \"orange\"):\n",
    "    os.makedirs(output_dir + \"orange/GT\")\n",
    "\n",
    "if not os.path.exists(output_dir + \"green\"):\n",
    "    os.makedirs(output_dir + \"green/GT\")\n",
    "\n",
    "if not os.path.exists(output_dir + \"orange/RAW\"):\n",
    "    os.makedirs(output_dir + \"orange/RAW\")\n",
    "\n",
    "if not os.path.exists(output_dir + \"green/RAW\"):\n",
    "    os.makedirs(output_dir + \"green/RAW\")\n",
    "\n",
    "\n",
    "orange_seq = pims.open(orange)\n",
    "green_seq = pims.open(green)\n",
    "Z_slices = 19\n",
    "xdim = 1200\n",
    "ydim = 1200\n",
    "\n",
    "timestep = 0\n",
    "counter = 0\n",
    "for t, (img_o, img_g) in enumerate(zip(orange_seq,green_seq)):\n",
    "    print(\"Timestep: \", timestep)\n",
    "\n",
    "    # iterate through all z slices and stack them together\n",
    "    if np.mod(t,Z_slices) == 0:\n",
    "        # prepare empty array with the same dtype as the first image\n",
    "        counter = 0\n",
    "        orange_zstack = np.zeros((Z_slices,xdim,ydim))\n",
    "        green_zstack = np.zeros((Z_slices,xdim,ydim))\n",
    "\n",
    "    orange_zstack[counter,:,:] = img_o\n",
    "    green_zstack[counter,:,:] = img_g\n",
    "\n",
    "    if np.mod(t,Z_slices) == Z_slices-1:\n",
    "\n",
    "        # now there is a 3D image stack of 19 slices\n",
    "        print(orange_zstack.shape)\n",
    "        print(green_zstack.shape)\n",
    "\n",
    "        # start napari, save GT and RAW images\n",
    "        # keep the basename of fn\n",
    "        fn_orange = os.path.basename(fn_orange)\n",
    "        fn_orange = os.path.splitext(fn_orange)[0]\n",
    "\n",
    "        Draw_and_Save(orange_zstack, output_dir + \"orange/GT\", fn_orange + '_T_'+str(timestep).zfill(3)+'.tif')\n",
    "        imsave(output_dir + \"orange/RAW/\" + fn_orange + '_T_'+str(timestep).zfill(3)+'.tif', orange_zstack,check_contrast=False)\n",
    "\n",
    "        Draw_and_Save(green_zstack, output_dir + \"green/GT\", fn_green + '_T_'+str(timestep).zfill(3)+'.tif')\n",
    "        imsave(output_dir + \"green/RAW/\" + fn_green + '_T_'+str(timestep).zfill(3)+'.tif', green_zstack,check_contrast=False)\n",
    "\n",
    "\n",
    "        timestep += 1\n",
    "\n",
    "    counter += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f9f2d",
   "metadata": {},
   "source": [
    "# Now work with the GT to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1d2d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Now go through all existing GT files that contain non-zero pixels\n",
    "green_gt_fn = os.listdir(output_dir + \"green/GT\")\n",
    "orange_gt_fn = os.listdir(output_dir + \"orange/GT\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-application",
   "metadata": {
    "id": "touched-application"
   },
   "source": [
    "## Generating a feature stack\n",
    "Pixel classifiers such as the random forest classifier takes multiple images as input. We typically call these images a feature stack because for every pixel exist now multiple values (features). In the following example we create a feature stack containing three features:\n",
    "* The original pixel value\n",
    "* The pixel value after a Gaussian blur\n",
    "* The pixel value of the Gaussian blurred image processed through a Sobel operator.\n",
    "\n",
    "Thus, we denoise the image and detect edges. All three images serve the pixel classifier to differentiate positive an negative pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "liberal-monster",
   "metadata": {
    "id": "liberal-monster",
    "outputId": "9ab5e9d7-1c36-4c37-bec9-ab69c39289a0"
   },
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "def generate_feature_stack(image):\n",
    "    # determine features\n",
    "    blurred = filters.gaussian(image, sigma=2)\n",
    "    edges = filters.sobel(blurred)\n",
    "\n",
    "    # collect features in a stack\n",
    "    # The ravel() function turns a nD image into a 1-D image.\n",
    "    # We need to use it because scikit-learn expects values in a 1-D format here.\n",
    "    feature_stack = [\n",
    "        image.ravel(),\n",
    "        blurred.ravel(),\n",
    "        edges.ravel()\n",
    "    ]\n",
    "\n",
    "    # return stack as numpy-array\n",
    "    return np.asarray(feature_stack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-english",
   "metadata": {
    "id": "painful-english"
   },
   "source": [
    "## Formating data\n",
    "We now need to format the input data so that it fits to what scikit learn expects. Scikit-learn asks for an array of shape (n, m) as input data and (n) annotations. n corresponds to number of pixels and m to number of features. In our case m = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "plastic-botswana",
   "metadata": {
    "id": "plastic-botswana",
    "outputId": "0516f370-4779-4fa0-b537-dba5ac615b2d"
   },
   "outputs": [],
   "source": [
    "def format_data(feature_stack, annotation):\n",
    "    # reformat the data to match what scikit-learn expects\n",
    "    # transpose the feature stack\n",
    "    X = feature_stack.T\n",
    "    # make the annotation 1-dimensional\n",
    "    y = annotation.ravel()\n",
    "\n",
    "    # remove all pixels from the feature and annotations which have not been annotated\n",
    "    mask = y > 0\n",
    "    X = X[mask]\n",
    "    y = y[mask]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a509b661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orange_20230920_AirGel_WTmSc_dwspFmN_mix_LAM02J-L_CIP_5.5hpi_23.5h.nd...el_WTmSc_dwspFmN_mix_LAM02J-L_CIP_5.5hpi_23.5h.nd2 (series 08) - C=1.tif_T_000.tif\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# in case you have not much memory as I do, you should run only one channel at a time\n",
    "\n",
    "channels = ['orange', 'green']\n",
    "filename_array = [orange_gt_fn, green_gt_fn]\n",
    "\n",
    "\n",
    "# make this to a for loop later\n",
    "channel = channels[0]\n",
    "filenames = filename_array[0]\n",
    "\n",
    "\n",
    "X_stack = []\n",
    "y_stack = []\n",
    "\n",
    "for fn in filenames[:1]:\n",
    "    print(fn)\n",
    "\n",
    "    img = imread(output_dir + channel + \"/RAW/\" + fn)\n",
    "    feature_stack = generate_feature_stack(img)\n",
    "    X, y = format_data(feature_stack, img)\n",
    "   \n",
    "    X_stack.append(X)\n",
    "    y_stack.append(y)\n",
    "\n",
    "    # delete variables to save memory\n",
    "    del X, y, img, feature_stack\n",
    "    \n",
    "X_stack = np.concatenate(X_stack)\n",
    "y_stack = np.concatenate(y_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c70d443d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27360000,), (27360000, 3))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_stack.shape, X_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73bb90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_stack = generate_feature_stack(mutant)\n",
    "\n",
    "# # show feature images\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig,axes = plt.subplots(1, 3, figsize=(10,10))\n",
    "\n",
    "\n",
    "# %gui qt\n",
    "# # reshape(image.shape) is the opposite of ravel() here. We just need it for visualization.\n",
    "# axes[0].imshow(feature_stack[0].reshape(mutant.shape), cmap=plt.cm.gray)\n",
    "# axes[1].imshow(feature_stack[1].reshape(mutant.shape), cmap=plt.cm.gray)\n",
    "# axes[2].imshow(feature_stack[2].reshape(mutant.shape), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-advantage",
   "metadata": {
    "id": "entitled-advantage"
   },
   "source": [
    "## Interactive segmentation\n",
    "We can also use napari to annotate some regions as negative (label = 1) and positive (label = 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-throat",
   "metadata": {
    "id": "strong-throat"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-harvey",
   "metadata": {
    "id": "institutional-harvey"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-figure",
   "metadata": {
    "id": "crude-figure"
   },
   "source": [
    "# Training begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e2645",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'transwell_denoised_2_categories.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ea0007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "phantom-papua",
   "metadata": {
    "id": "phantom-papua",
    "outputId": "8a923b7a-d366-4d08-eb99-5bdd073e67b1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1613ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train classifier if not trained yet\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_stack, y_stack)\n",
    "\n",
    "# # save classifier\n",
    "# import pickle\n",
    "# pickle.dump(classifier, open(filename, 'wb'))\n",
    "\n",
    "# # make prediction\n",
    "# prediction = classifier.predict(feature_stack.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059c67ec",
   "metadata": {},
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42dd2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50,100],  # Vary the number of trees\n",
    "    'max_depth': [2, 3],       # Vary the maximum depth of trees\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5)\n",
    "grid_search.fit(X, y)  # X and y are your training data and labels, respectively\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19275c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search.cv_results_\n",
    "\n",
    "# Extract the mean scores and reshape them into a grid\n",
    "scores = np.array(results['mean_test_score']).reshape(len(param_grid['n_estimators']),\n",
    "                                                      len(param_grid['max_depth']))\n",
    "\n",
    "\n",
    "\n",
    "# Create a heatmap of the mean scores\n",
    "plt.imshow(scores, cmap='viridis', origin='lower')\n",
    "plt.colorbar(label='Mean Score')\n",
    "plt.xlabel('min_samples_split')\n",
    "plt.ylabel('max_depth')\n",
    "plt.title('Grid Search Mean Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ccaee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9b1652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save classifier \n",
    "import pickle\n",
    "pickle.dump(best_classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8a553b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2ec2778",
   "metadata": {},
   "source": [
    "# Now go to the other notebook for prediction. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
